# Copyright (c) 2022-2025, The Isaac Lab Project Developers
# SPDX-License-Identifier: BSD-3-Clause

"""
Script to play a checkpoint of an RL agent from skrl for RobotArm environment.

Usage:
    python play.py --task RobotArm-v0 --num_envs 1 --checkpoint /path/to/checkpoint.pt
"""

import argparse
import sys
import os
import random
import time
import torch

# [1] Isaac Sim App Launcher
from isaaclab.app import AppLauncher

# 인자 파싱
parser = argparse.ArgumentParser(description="Play a checkpoint of an RL agent from skrl.")
parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
parser.add_argument("--video_length", type=int, default=200, help="Length of the recorded video (in steps).")
parser.add_argument(
    "--disable_fabric", action="store_true", default=False, help="Disable fabric and use USD I/O operations."
)
parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
parser.add_argument("--task", type=str, default="RobotArm-v0", help="Name of the task.")
parser.add_argument(
    "--agent",
    type=str,
    default=None,
    help=(
        "Name of the RL agent configuration entry point. Defaults to None, in which case the argument "
        "--algorithm is used to determine the default agent configuration entry point."
    ),
)
parser.add_argument("--checkpoint", type=str, default=None, help="Path to model checkpoint.")
parser.add_argument("--seed", type=int, default=None, help="Seed used for the environment")
parser.add_argument(
    "--use_pretrained_checkpoint",
    action="store_true",
    help="Use the pre-trained checkpoint from Nucleus.",
)
parser.add_argument(
    "--ml_framework",
    type=str,
    default="torch",
    choices=["torch", "jax", "jax-numpy"],
    help="The ML framework used for training the skrl agent.",
)
parser.add_argument(
    "--algorithm",
    type=str,
    default="PPO",
    choices=["AMP", "PPO", "IPPO", "MAPPO"],
    help="The RL algorithm used for training the skrl agent.",
)
parser.add_argument("--real-time", action="store_true", default=False, help="Run in real-time, if possible.")

# AppLauncher 인자 추가 및 파싱
AppLauncher.add_app_launcher_args(parser)
args_cli, hydra_args = parser.parse_known_args()

# 비디오 녹화 시 카메라 활성화
if args_cli.video:
    args_cli.enable_cameras = True

# Hydra argv 정리
sys.argv = [sys.argv[0]] + hydra_args

# [2] 시뮬레이터 앱 실행
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

# ----------------------------------------------------------------------
# 이후 라이브러리 임포트
# ----------------------------------------------------------------------

import gymnasium as gym
import skrl
from packaging import version

# SKRL 버전 체크
SKRL_VERSION = "1.4.3"
if version.parse(skrl.__version__) < version.parse(SKRL_VERSION):
    skrl.logger.error(
        f"Unsupported skrl version: {skrl.__version__}. "
        f"Install supported version using 'pip install skrl>={SKRL_VERSION}'"
    )
    exit()

# ML Framework Runner 선택
if args_cli.ml_framework.startswith("torch"):
    from skrl.utils.runner.torch import Runner
elif args_cli.ml_framework.startswith("jax"):
    from skrl.utils.runner.jax import Runner

# Isaac Lab 모듈 임포트
from isaaclab.envs import (
    DirectMARLEnv,
    DirectMARLEnvCfg,
    DirectRLEnvCfg,
    ManagerBasedRLEnvCfg,
    multi_agent_to_single_agent,
)
from isaaclab.utils.dict import print_dict
from isaaclab.utils.pretrained_checkpoint import get_published_pretrained_checkpoint
from isaaclab_rl.skrl import SkrlVecEnvWrapper

import isaaclab_tasks  # noqa: F401
from isaaclab_tasks.utils import get_checkpoint_path
from isaaclab_tasks.utils.hydra import hydra_task_config

# [중요] 사용자의 커스텀 Task 패키지 임포트
import RobotArm.tasks  # noqa: F401

# Config 설정 바로가기
if args_cli.agent is None:
    algorithm = args_cli.algorithm.lower()
    agent_cfg_entry_point = "skrl_cfg_entry_point" if algorithm in ["ppo"] else f"skrl_{algorithm}_cfg_entry_point"
else:
    agent_cfg_entry_point = args_cli.agent


@hydra_task_config(args_cli.task, agent_cfg_entry_point)
def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, experiment_cfg: dict):
    """Play with skrl agent."""
    
    # Task 이름 정리 (Checkpoint 경로 찾기용)
    task_name = args_cli.task.split(":")[-1]
    train_task_name = task_name.replace("-Play", "")

    # 1. 환경 설정 오버라이드
    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs
    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device

    # JAX 백엔드 설정
    if args_cli.ml_framework.startswith("jax"):
        skrl.config.jax.backend = "jax" if args_cli.ml_framework == "jax" else "numpy"

    # 시드 설정
    if args_cli.seed == -1:
        args_cli.seed = random.randint(0, 10000)
    experiment_cfg["seed"] = args_cli.seed if args_cli.seed is not None else experiment_cfg["seed"]
    env_cfg.seed = experiment_cfg["seed"]

    # 2. 체크포인트 경로 설정
    log_root_path = os.path.join("logs", "skrl", experiment_cfg["agent"]["experiment"]["directory"])
    log_root_path = os.path.abspath(log_root_path)
    print(f"[INFO] Loading experiment from directory: {log_root_path}")

    # (A) 사전 학습된 모델 사용 시
    if args_cli.use_pretrained_checkpoint:
        resume_path = get_published_pretrained_checkpoint("skrl", train_task_name)
        if not resume_path:
            print("[INFO] Unfortunately a pre-trained checkpoint is currently unavailable for this task.")
            return
    # (B) 직접 지정한 체크포인트 사용 시 (가장 많이 사용됨)
    elif args_cli.checkpoint:
        resume_path = os.path.abspath(args_cli.checkpoint)
    # (C) 최근 학습 기록에서 자동 탐색
    else:
        resume_path = get_checkpoint_path(
            log_root_path, run_dir=f".*_{algorithm}_{args_cli.ml_framework}", other_dirs=["checkpoints"]
        )

    if not resume_path or not os.path.exists(resume_path):
        print(f"[ERROR] Checkpoint not found at: {resume_path}")
        # 체크포인트 없이 실행할지 여부 결정 (보통은 종료)
        # return 
    
    log_dir = os.path.dirname(os.path.dirname(resume_path)) if resume_path else log_root_path

    # 3. 환경 생성
    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)

    # Multi-Agent 처리
    if isinstance(env.unwrapped, DirectMARLEnv) and algorithm in ["ppo"]:
        env = multi_agent_to_single_agent(env)

    # Real-time 실행을 위한 dt 계산
    try:
        dt = env.step_dt
    except AttributeError:
        dt = env.unwrapped.step_dt

    # 비디오 녹화 설정
    if args_cli.video:
        video_kwargs = {
            "video_folder": os.path.join(log_dir, "videos", "play"),
            "step_trigger": lambda step: step == 0,
            "video_length": args_cli.video_length,
            "disable_logger": True,
        }
        print("[INFO] Recording videos during play.")
        print_dict(video_kwargs, nesting=4)
        env = gym.wrappers.RecordVideo(env, **video_kwargs)

    # SKRL 래퍼 적용
    env = SkrlVecEnvWrapper(env, ml_framework=args_cli.ml_framework)

    # 4. Runner 설정 및 모델 로드
    # Play 모드에서는 학습 관련 설정 끄기
    experiment_cfg["trainer"]["close_environment_at_exit"] = False
    experiment_cfg["agent"]["experiment"]["write_interval"] = 0 
    experiment_cfg["agent"]["experiment"]["checkpoint_interval"] = 0 
    
    runner = Runner(env, experiment_cfg)

    if resume_path:
        print(f"[INFO] Loading model checkpoint from: {resume_path}")
        runner.agent.load(resume_path)
    
    # 평가 모드로 전환 (Dropout 등 비활성화)
    runner.agent.set_running_mode("eval")

    # 5. 시뮬레이션 루프
    obs, _ = env.reset()
    timestep = 0
    
    while simulation_app.is_running():
        start_time = time.time()

        with torch.inference_mode():
            # Action 계산 (Deterministic=True 권장)
            outputs = runner.agent.act(obs, timestep=0, timesteps=0)
            
            # Action 추출
            if hasattr(env, "possible_agents"):
                # Multi-agent
                actions = {a: outputs[-1][a].get("mean_actions", outputs[0][a]) for a in env.possible_agents}
            else:
                # Single-agent
                # mean_actions를 사용하여 확률적 요소 제거 (탐험 X)
                actions = outputs[-1].get("mean_actions", outputs[0])
            
            # 환경 스텝
            obs, _, _, _, _ = env.step(actions)
        
        # 비디오 녹화 종료 체크
        if args_cli.video:
            timestep += 1
            if timestep == args_cli.video_length:
                break

        # Real-time 동기화 (너무 빠르면 대기)
        sleep_time = dt - (time.time() - start_time)
        if args_cli.real_time and sleep_time > 0:
            time.sleep(sleep_time)

    # 종료
    env.close()


if __name__ == "__main__":
    main()
    simulation_app.close()
